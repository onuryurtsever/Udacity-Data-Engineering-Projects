{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Lake for U.S. Immigration Office\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project we are creating data lake about U.S. Immigrants and their behaviours which are related to city, temperature, airport.\n",
    "\n",
    "Also we aim to ask some questions such as below:\n",
    "\n",
    "* Why immigrants do choose these cities?\n",
    "* Do they like mostly hot or warmer cities?\n",
    "* Demographic structure is reason to choose these cities?\n",
    "\n",
    "Therefore U.S. Immigrant Office will be able to take better decisions in their policies.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Our primary target is to create fact and dimension tables for data lake from different data sources and find answers of questions above.\n",
    "\n",
    "After gathering data from data sources, first we wil store them in parquet files under directory which is \"data_source/\"\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### 1 - I94 Immigration Data\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project. [Link](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "\n",
    "##### Data Dictionary for I94 Immigration Data\n",
    "* **CICID:** Unique ID\n",
    "* **I94YR:** 4 digit year\n",
    "* **I94MON:** Numeric month\n",
    "* **I94CIT:** Country of citizenship\n",
    "* **I94RES:** Country of residence\n",
    "* **I94PORT:** Port addmitted through\n",
    "* **ARRDATE:** is the Arrival Date in the USA. It is a SAS date numeric field that a permament format has not been applied.\n",
    "* **I94MODE:** 1 = Air; 2 = Sea; 3 = Land; 9 = Not reported\n",
    "* **I94ADDR:** State of arrival\n",
    "* **DEPDATE:** is the Departure Date from the USA.\n",
    "* **I94BIR:** Age of Respondent in Years\n",
    "* **I94VISA:** Visa codes collapsed into three categories 1 = Business, 2 = Pleasure, 3 = Student\n",
    "* **COUNT:** Used for summary statistics\n",
    "* **DTADFILE:** Character Date Field - Date added to I-94 Files\n",
    "* **VISAPOST:** Department of State where where Visa was issued\n",
    "* **OCCUP:** Occupation that will be performed in U.S.\n",
    "* **ENTDEPA:** Arrival Flag - admitted or paroled into the U.S.\n",
    "* **ENTDEPD:** Departure Flag - Departed, lost I-94 or is deceased\n",
    "* **ENTDEPU:** Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "* **MATFLAG:** Match flag - Match of arrival and departure records\n",
    "* **BIRYEAR:** 4 digit year of birth\n",
    "* **DTADDTO:** Character Date Field - Date to which admitted to U.S. (allowed to stay until)\n",
    "* **GENDER:** Non-immigrant sex\n",
    "* **INSNUM:** INS number\n",
    "* **AIRLINE:** Airline used to arrive in U.S.\n",
    "* **ADMNUM:** Admission Number\n",
    "* **FLTNO:** Flight number of Airline used to arrive in U.S.\n",
    "* **VISATYPE:** Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "\n",
    "##### 2- World Temperature Data\n",
    "This dataset came from Kaggle. You can read more about it here. [Link](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "##### Data Dictionary for World Temperature Data\n",
    "* **dt**\n",
    "* **AverageTemperature**\n",
    "* **AverageTemperatureUncertainty**\n",
    "* **City**\n",
    "* **Country**\n",
    "* **Latitude**\n",
    "* **Longitude**\n",
    "\n",
    "##### 3- U.S. City Demographic Data\n",
    "This data comes from OpenSoft. You can read more about it here. [Link](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "##### Data Dictionary for U.S. City Demographic Data\n",
    "* **City**\n",
    "* **State**\n",
    "* **Median Age**\n",
    "* **Male Population**\n",
    "* **Female Population**\n",
    "* **Total Population**\n",
    "* **Number of Veterans**\n",
    "* **Foreign-born**\n",
    "* **Average Household Size**\n",
    "* **State Code**\n",
    "* **Race**\n",
    "* **Count**\n",
    "\n",
    "\n",
    "##### 4- Airport Code Table\n",
    "This is a simple table of airport codes and corresponding cities. It comes from here. [Link](https://datahub.io/core/airport-codes#data)\n",
    "##### Data Dictionary for Airport Code Table\n",
    "* **ident**\n",
    "* **type**\n",
    "* **name**\n",
    "* **elevation_ft**\n",
    "* **continent**\n",
    "* **iso_country**\n",
    "* **iso_region**\n",
    "* **municipality**\n",
    "* **gps_code**\n",
    "* **iata_code**\n",
    "* **local_code**\n",
    "* **coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration = spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration\")\n",
    "df_immigration = spark.sql(\"SELECT * FROM immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.write.mode('overwrite').parquet(\"data_source/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_parquet = spark.read.parquet(\"data_source/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp = spark.read.format('csv').option(\"delimiter\", \",\").option(\"header\", \"true\").load('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "df_temp.createOrReplaceTempView(\"temperature\")\n",
    "df_temp = spark.sql(\"SELECT * FROM temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp.write.mode('overwrite').parquet(\"data_source/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_parquet = spark.read.parquet(\"data_source/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes = spark.read.format('csv').option(\"delimiter\", \",\").option(\"header\", \"true\").load('airport-codes_csv.csv')\n",
    "df_airport_codes.createOrReplaceTempView(\"airport_codes\")\n",
    "df_airport_codes = spark.sql(\"SELECT * FROM airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.write.mode('overwrite').parquet(\"data_source/airport_codes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_parquet = spark.read.parquet(\"data_source/airport_codes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_codes_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demography = spark.read.format('csv').option(\"delimiter\", \";\").option(\"header\", \"true\").load('us-cities-demographics.csv')\n",
    "df_demography.createOrReplaceTempView(\"demography\")\n",
    "df_demography = spark.sql(\"SELECT City, State, `Median Age` AS Median_Age, `Male Population` AS Male_Population,\\\n",
    "                          `Total Population` AS Total_Population, `Number of Veterans` AS Number_of_Veterans,\\\n",
    "                          `Foreign-born` AS Foreign_Born, `Average Household Size` AS Average_Household_Size,\\\n",
    "                          `State Code` AS State_Code, Race, Count \\\n",
    "                          FROM demography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median_Age|Male_Population|Total_Population|Number_of_Veterans|Foreign_Born|Average_Household_Size|State_Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demography.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demography.write.mode('overwrite').parquet(\"data_source/demography.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demography_parquet = spark.read.parquet(\"data_source/demography.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demography.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demography_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|addr_id| addr_desc|\n",
      "+-------+----------+\n",
      "|     AL|   ALABAMA|\n",
      "|     AK|    ALASKA|\n",
      "|     AZ|   ARIZONA|\n",
      "|     AR|  ARKANSAS|\n",
      "|     CA|CALIFORNIA|\n",
      "+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting data from sas label data and storing as parquet file\n",
    "df_addr_label = spark.read.format('csv').option(\"delimiter\", \";\").option(\"header\", \"true\").load('address.csv')\n",
    "df_addr_label.write.mode('overwrite').parquet(\"stage_source/stg_address.parquet\")\n",
    "df_addr_label = spark.read.parquet(\"stage_source/stg_address.parquet\")\n",
    "df_addr_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+\n",
      "|airport_id|        airport_city|airport_state|\n",
      "+----------+--------------------+-------------+\n",
      "|       ALC|               ALCAN|           AK|\n",
      "|       ANC|           ANCHORAGE|           AK|\n",
      "|       BAR|BAKER AAF - BAKER...|           AK|\n",
      "|       DAC|       DALTONS CACHE|           AK|\n",
      "|       PIZ|DEW STATION PT LA...|           AK|\n",
      "+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_label = spark.read.format('csv').option(\"delimiter\", \";\").option(\"header\", \"true\").load('airport.csv')\n",
    "df_airport_label.write.mode('overwrite').parquet(\"stage_source/stg_airport.parquet\")\n",
    "df_airport_label = spark.read.parquet(\"stage_source/stg_airport.parquet\")\n",
    "df_airport_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|country_id|        country_desc|\n",
      "+----------+--------------------+\n",
      "|       582|  MEXICO Air Sea,...|\n",
      "|       236|         AFGHANISTAN|\n",
      "|       101|             ALBANIA|\n",
      "|       316|             ALGERIA|\n",
      "|       102|             ANDORRA|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_label = spark.read.format('csv').option(\"delimiter\", \";\").option(\"header\", \"true\").load('country.csv')\n",
    "df_country_label.write.mode('overwrite').parquet(\"stage_source/stg_country.parquet\")\n",
    "df_country_label = spark.read.parquet(\"stage_source/stg_country.parquet\")\n",
    "df_country_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|mode_id|   mode_desc|\n",
      "+-------+------------+\n",
      "|      1|         Air|\n",
      "|      2|         Sea|\n",
      "|      3|        Land|\n",
      "|      9|Not reported|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mode_label = spark.read.format('csv').option(\"delimiter\", \";\").option(\"header\", \"true\").load('mode.csv')\n",
    "df_mode_label.write.mode('overwrite').parquet(\"stage_source/stg_mode.parquet\")\n",
    "df_mode_label = spark.read.parquet(\"stage_source/stg_mode.parquet\")\n",
    "df_mode_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|visa_id|visa_desc|\n",
      "+-------+---------+\n",
      "|      1| Business|\n",
      "|      2| Pleasure|\n",
      "|      3|  Student|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visa_label = spark.read.format('csv').option(\"delimiter\", \";\").option(\"header\", \"true\").load('visa.csv')\n",
    "df_visa_label.write.mode('overwrite').parquet(\"stage_source/stg_visa.parquet\")\n",
    "df_visa_label = spark.read.parquet(\"stage_source/stg_visa.parquet\")\n",
    "df_visa_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Temperature Data\n",
    "df_temp = spark.read.parquet(\"data_source/temperature.parquet\")\n",
    "df_temp.createOrReplaceTempView(\"temperature\")\n",
    "df_temp = spark.sql(\"SELECT dt, AverageTemperature, AverageTemperatureUncertainty, \\\n",
    "                    UPPER(City) AS City, UPPER(Country) AS Country, \\\n",
    "                    Latitude,Longitude \\\n",
    "                    FROM temperature WHERE Country='United States' AND YEAR(dt)=2013 AND (AverageTemperature IS NOT NULL OR AverageTemperatureUncertainty IS NOT NULL)\")\n",
    "df_temp = df_temp.dropDuplicates(['dt', 'City'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------------------+------------+-------------+--------+---------+\n",
      "|        dt|  AverageTemperature|AverageTemperatureUncertainty|        City|      Country|Latitude|Longitude|\n",
      "+----------+--------------------+-----------------------------+------------+-------------+--------+---------+\n",
      "|2013-02-01|0.053999999999999986|                        0.198|INDEPENDENCE|UNITED STATES|  39.38N|   93.64W|\n",
      "|2013-03-01|              12.543|                        0.225|   ARLINGTON|UNITED STATES|  32.95N|   96.70W|\n",
      "|2013-03-01|  13.505999999999998|                        0.325| SANTA CLARA|UNITED STATES|  37.78N|  122.03W|\n",
      "|2013-04-01|  6.8359999999999985|                         0.39|     LANSING|UNITED STATES|  42.59N|   85.09W|\n",
      "|2013-05-01|              14.309|                        0.331|  MANCHESTER|UNITED STATES|  42.59N|   72.00W|\n",
      "+----------+--------------------+-----------------------------+------------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               0\n",
       "AverageTemperature               0\n",
       "AverageTemperatureUncertainty    0\n",
       "City                             0\n",
       "Country                          0\n",
       "Latitude                         0\n",
       "Longitude                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_pandas = df_temp.toPandas()\n",
    "df_temp_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp.write.mode('overwrite').parquet(\"stage_source/stg_temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Airport Data\n",
    "df_airport_codes = spark.read.parquet(\"data_source/airport_codes.parquet\")\n",
    "df_airport_codes.createOrReplaceTempView(\"airport_codes\")\n",
    "df_airport_codes = spark.sql(\"SELECT ident, type, UPPER(name) AS name, elevation_ft, continent, iso_country,\\\n",
    "                        iso_region, UPPER(municipality) AS municipality, gps_code, iata_code, local_code,\\\n",
    "                        coordinates \\\n",
    "                       FROM airport_codes WHERE iso_country='US' AND type IN ('large_airport','medium_airport','small_airport')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "| 00AA|small_airport|AERO B RANCH AIRPORT|        3435|       NA|         US|     US-KS|       LEOTI|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        LOWELL FIELD|         450|       NA|         US|     US-AK|ANCHOR POINT|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        EPPS AIRPARK|         820|       NA|         US|     US-AL|     HARVEST|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AS|small_airport|      FULTON AIRPORT|        1100|       NA|         US|     US-OK|        ALEX|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      CORDES AIRPORT|        3810|       NA|         US|     US-AZ|      CORDES|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft       63\n",
       "continent           0\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality       50\n",
       "gps_code          399\n",
       "iata_code       12717\n",
       "local_code        199\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_pandas = df_airport_codes.toPandas()\n",
    "df_airport_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.write.mode('overwrite').parquet(\"stage_source/stg_airport_codes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Demographic Data\n",
    "df_demography = spark.read.parquet(\"data_source/demography.parquet\")\n",
    "df_demography.createOrReplaceTempView(\"demography\")\n",
    "df_demography = spark.sql(\"SELECT UPPER(City) AS City, UPPER(State) AS State, \\\n",
    "                        Median_Age, Male_Population, Total_Population, Number_of_Veterans, \\\n",
    "                        Foreign_Born, Average_Household_Size, State_Code, Race, Count \\\n",
    "                        FROM demography WHERE Race IS NOT NULL OR City IS NOT NULL OR State_Code IS NOT NULL OR State IS NOT NULL\")\n",
    "df_demography = df_demography.dropDuplicates(['City', 'State', 'State_Code', 'Race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|                City|     State|Median_Age|Male_Population|Total_Population|Number_of_Veterans|Foreign_Born|Average_Household_Size|State_Code|                Race|Count|\n",
      "+--------------------+----------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|        APPLE VALLEY|CALIFORNIA|      34.3|          32873|           72185|              5714|        5801|                  3.03|        CA|  Hispanic or Latino|25928|\n",
      "|ATHENS-CLARKE COU...|   GEORGIA|      26.5|          57415|          122563|              3953|       12868|                  2.44|        GA|  Hispanic or Latino|13159|\n",
      "|            BERKELEY|CALIFORNIA|      32.5|          60142|          120971|              3736|       25000|                  2.35|        CA|American Indian a...| 1868|\n",
      "|            BERKELEY|CALIFORNIA|      32.5|          60142|          120971|              3736|       25000|                  2.35|        CA|  Hispanic or Latino|15429|\n",
      "|        FAYETTEVILLE|  ARKANSAS|      27.1|          41959|           82832|              4744|        6313|                  2.28|        AR|Black or African-...| 6927|\n",
      "+--------------------+----------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demography.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demography_pandas = df_demography.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median_Age                 0\n",
       "Male_Population            3\n",
       "Total_Population           0\n",
       "Number_of_Veterans        13\n",
       "Foreign_Born              13\n",
       "Average_Household_Size    16\n",
       "State_Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demography_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demography.write.mode('overwrite').parquet(\"stage_source/stg_demography.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cleaning Immigrant Data\n",
    "df_immigrant = spark.read.parquet(\"data_source/immigration.parquet\")\n",
    "df_immigrant.createOrReplaceTempView(\"immigration\")\n",
    "df_addr_label.createOrReplaceTempView(\"addr_label\")\n",
    "df_airport_label.createOrReplaceTempView(\"airport_label\")\n",
    "df_country_label.createOrReplaceTempView(\"country_label\")\n",
    "df_mode_label.createOrReplaceTempView(\"mode_label\")\n",
    "df_visa_label.createOrReplaceTempView(\"visa_label\")\n",
    "\n",
    "df_immigrant = spark.sql(\"SELECT cicid AS Record_ID, i94yr AS Year, i94mon AS Month, \\\n",
    "                         CIT.country_desc AS Country_of_Citizen, RES.country_desc AS Country_of_Residence, \\\n",
    "                         AIR.airport_id AS Airport, date_add(to_date('1960-01-01'), arrdate) AS Arrival_Date, MOD.mode_desc AS Transfer_Mode,\\\n",
    "                         ADR.addr_desc AS Address, date_add(to_date('1960-01-01'), depdate) AS Departure_Date, i94bir AS Age, VIS.visa_desc AS Visa, \\\n",
    "                         biryear AS Birth_Year, gender AS Gender, airline AS Airline, visatype AS Visa_Type, \\\n",
    "                         UPPER(AIR.airport_city) AS City, UPPER(AIR.airport_state) AS State, count AS Number_of_Arrivals  \\\n",
    "                         FROM immigration F \\\n",
    "                         INNER JOIN country_label CIT ON F.i94cit=CIT.country_id \\\n",
    "                         INNER JOIN country_label RES ON F.i94res=RES.country_id \\\n",
    "                         INNER JOIN airport_label AIR ON F.i94port=AIR.airport_id \\\n",
    "                         INNER JOIN addr_label ADR ON F.i94addr=ADR.addr_id \\\n",
    "                         INNER JOIN mode_label MOD ON F.i94mode=MOD.mode_id \\\n",
    "                         INNER JOIN visa_label VIS ON F.i94visa=VIS.visa_id \\\n",
    "                         WHERE MOD.mode_desc='Air' AND F.gender IN ('F','M') \\\n",
    "                         AND F.i94bir IS NOT NULL AND F.airline IS NOT NULL \\\n",
    "                         AND UPPER(AIR.airport_state) IS NOT NULL\")\n",
    "                            \n",
    "df_immigrant = df_immigrant.dropDuplicates(['Record_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+------------------+--------------------+-------+------------+-------------+-----------+--------------+----+--------+----------+------+-------+---------+----------------+-------+------------------+\n",
      "|Record_ID|  Year|Month|Country_of_Citizen|Country_of_Residence|Airport|Arrival_Date|Transfer_Mode|    Address|Departure_Date| Age|    Visa|Birth_Year|Gender|Airline|Visa_Type|            City|  State|Number_of_Arrivals|\n",
      "+---------+------+-----+------------------+--------------------+-------+------------+-------------+-----------+--------------+----+--------+----------+------+-------+---------+----------------+-------+------------------+\n",
      "|    558.0|2016.0|  4.0|           AUSTRIA|             AUSTRIA|    SFR|  2016-04-01|          Air| CALIFORNIA|    2016-04-03|42.0|Business|    1974.0|     M|     LH|       WB|   SAN FRANCISCO|     CA|               1.0|\n",
      "|    596.0|2016.0|  4.0|           AUSTRIA|             AUSTRIA|    NAS|  2016-04-01|          Air|    FLORIDA|    2016-04-03|24.0|Pleasure|    1992.0|     M|     UP|       WT|          NASSAU|BAHAMAS|               1.0|\n",
      "|    934.0|2016.0|  4.0|           BELGIUM|             BELGIUM|    NEW|  2016-04-01|          Air|   NEW YORK|    2016-04-05|54.0|Pleasure|    1962.0|     F|     UA|       WT|NEWARK/TETERBORO|     NJ|               1.0|\n",
      "|   1051.0|2016.0|  4.0|           BELGIUM|             BELGIUM|    NEW|  2016-04-01|          Air|   NEW YORK|    2016-04-07|28.0|Pleasure|    1988.0|     M|     LH|       WT|NEWARK/TETERBORO|     NJ|               1.0|\n",
      "|   2734.0|2016.0|  4.0|            POLAND|              POLAND|    SAJ|  2016-04-01|          Air|PUERTO RICO|    2016-04-08| 6.0|Pleasure|    2010.0|     M|     DY|       B2|        SAN JUAN|     PR|               1.0|\n",
      "+---------+------+-----+------------------+--------------------+-------+------------+-------------+-----------+--------------+----+--------+----------+------+-------+---------+----------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigrant.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigrant_pandas = df_immigrant.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Record_ID                   0\n",
       "Year                        0\n",
       "Month                       0\n",
       "Country_of_Citizen          0\n",
       "Country_of_Residence        0\n",
       "Airport                     0\n",
       "Arrival_Date                0\n",
       "Transfer_Mode               0\n",
       "Address                     0\n",
       "Departure_Date          90647\n",
       "Age                         0\n",
       "Visa                        0\n",
       "Birth_Year                  0\n",
       "Gender                      0\n",
       "Airline                     0\n",
       "Visa_Type                   0\n",
       "city                        0\n",
       "state                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigrant_pandas.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigrant.write.mode('overwrite').parquet(\"stage_source/stg_immigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Star Schema is the selected model which is easy and effective to create queries by joining fact and dimension tables for analysing data.\n",
    "\n",
    "##### Stage Tables\n",
    "\n",
    "* stg_temperature\n",
    "* stg_demoghraphy\n",
    "* stg_airport\n",
    "* stg_immigration\n",
    "\n",
    "##### Dimension Tables\n",
    "\n",
    "* dim_temperature\n",
    "* dim_demography\n",
    "* dim_airport\n",
    "* dim_time\n",
    "\n",
    "##### Fact Tables\n",
    "* fct_immigration\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "* Gathering data from several resources and stored under directory which is \"data_source\".\n",
    "* Stage tables are created from data sources under directory which is \"stage_source\".\n",
    "* null, duplicated values are removed from data sources.\n",
    "* Aggregation is made if needed.\n",
    "* Fact and dimension tables are created in parquet format under directory which is \"fact_dimension\" for data_lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigrant = spark.read.parquet(\"stage_source/stg_immigration.parquet\")\n",
    "df_temperature = spark.read.parquet(\"stage_source/stg_temperature.parquet\")\n",
    "df_airport = spark.read.parquet(\"stage_source/stg_airport_codes.parquet\")\n",
    "df_demography = spark.read.parquet(\"stage_source/stg_demography.parquet\")\n",
    "\n",
    "df_immigrant.createOrReplaceTempView(\"immigration\")\n",
    "df_temperature.createOrReplaceTempView(\"temperature\")\n",
    "df_airport.createOrReplaceTempView(\"airport\")\n",
    "df_demography.createOrReplaceTempView(\"demography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_time\n",
    "df_time = spark.sql(\"\"\"select distinct arrival_date as start_time, \n",
    "                                        hour(arrival_date) as hour, \n",
    "                                        day(arrival_date) as day, \n",
    "                                        weekofyear(arrival_date) as week, \n",
    "                                        month(arrival_date) as month, \n",
    "                                        year(arrival_date) as year, \n",
    "                                        dayofweek(arrival_date) as weekday \n",
    "                           from immigration\n",
    "                       \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+----+-----+----+-------+\n",
      "|start_time|hour|day|week|month|year|weekday|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "|2016-04-29|   0| 29|  17|    4|2016|      6|\n",
      "|2016-04-04|   0|  4|  14|    4|2016|      2|\n",
      "|2016-04-05|   0|  5|  14|    4|2016|      3|\n",
      "|2016-04-15|   0| 15|  15|    4|2016|      6|\n",
      "|2016-04-27|   0| 27|  17|    4|2016|      4|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_time.write.mode('overwrite').parquet(\"fact_dimension/dim_time.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_temperature\n",
    "df_temperature = spark.sql(\"SELECT City, AVG(AverageTemperature) AS Average_Temperature FROM temperature GROUP BY City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|        City|Average_Temperature|\n",
      "+------------+-------------------+\n",
      "|    STAMFORD| 12.163888888888888|\n",
      "|    AMARILLO|  17.52955555555555|\n",
      "|    SAVANNAH| 20.969111111111108|\n",
      "|     ORLANDO|  23.77077777777778|\n",
      "|INDIANAPOLIS| 13.398333333333333|\n",
      "+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Average_Temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature.write.mode('overwrite').parquet(\"fact_dimension/dim_temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_airport\n",
    "df_airport = spark.sql(\"SELECT * FROM airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "| 00AA|small_airport|AERO B RANCH AIRPORT|        3435|       NA|         US|     US-KS|       LEOTI|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        LOWELL FIELD|         450|       NA|         US|     US-AK|ANCHOR POINT|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        EPPS AIRPARK|         820|       NA|         US|     US-AL|     HARVEST|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AS|small_airport|      FULTON AIRPORT|        1100|       NA|         US|     US-OK|        ALEX|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      CORDES AIRPORT|        3810|       NA|         US|     US-AZ|      CORDES|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport.write.mode('overwrite').parquet(\"fact_dimension/dim_airport.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_demography\n",
    "df_demography = spark.sql(\"SELECT * FROM demography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median_Age|Male_Population|Total_Population|Number_of_Veterans|Foreign_Born|Average_Household_Size|State_Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|          AUBURN|    WASHINGTON|      37.1|          36837|           76580|              5401|       14842|                  2.73|        WA|Black or African-...|  4032|\n",
      "|           BOISE|         IDAHO|      34.9|         110099|          218280|             16004|       13409|                  2.61|        ID|               White|204913|\n",
      "|         BRANDON|       FLORIDA|      36.1|          55679|          113968|              9417|       16390|                  2.64|        FL|               Asian|  9440|\n",
      "|COLORADO SPRINGS|      COLORADO|      34.8|         225544|          456562|             49291|       35320|                  2.48|        CO|American Indian a...| 11146|\n",
      "|         CONCORD|NORTH CAROLINA|      35.7|          42732|           87693|              4621|        8847|                  2.72|        NC|  Hispanic or Latino| 11301|\n",
      "+----------------+--------------+----------+---------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demography.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median_Age: string (nullable = true)\n",
      " |-- Male_Population: string (nullable = true)\n",
      " |-- Total_Population: string (nullable = true)\n",
      " |-- Number_of_Veterans: string (nullable = true)\n",
      " |-- Foreign_Born: string (nullable = true)\n",
      " |-- Average_Household_Size: string (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demography.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demography.write.mode('overwrite').parquet(\"fact_dimension/dim_demography.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dim_immigration\n",
    "df_immigrant = spark.sql(\"SELECT * FROM immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+------------------+--------------------+-------+------------+-------------+----------+--------------+----+--------+----------+------+-------+---------+----------------+-----+------------------+\n",
      "|Record_ID|  Year|Month|Country_of_Citizen|Country_of_Residence|Airport|Arrival_Date|Transfer_Mode|   Address|Departure_Date| Age|    Visa|Birth_Year|Gender|Airline|Visa_Type|            City|State|Number_of_Arrivals|\n",
      "+---------+------+-----+------------------+--------------------+-------+------------+-------------+----------+--------------+----+--------+----------+------+-------+---------+----------------+-----+------------------+\n",
      "|    646.0|2016.0|  4.0|           AUSTRIA|             AUSTRIA|    CHI|  2016-04-01|          Air|  ILLINOIS|          null|37.0|Business|    1979.0|     M|     OS|       E2|         CHICAGO|   IL|               1.0|\n",
      "|    898.0|2016.0|  4.0|           BELGIUM|             BELGIUM|    NEW|  2016-04-01|          Air|NEW JERSEY|    2016-04-08|13.0|Pleasure|    2003.0|     M|     DL|       WT|NEWARK/TETERBORO|   NJ|               1.0|\n",
      "|   1177.0|2016.0|  4.0|           BELGIUM|             BELGIUM|    WAS|  2016-04-01|          Air|  VIRGINIA|    2016-04-10|56.0|Pleasure|    1960.0|     M|     UA|       WT|      WASHINGTON|   DC|               1.0|\n",
      "|   1322.0|2016.0|  4.0|           BELGIUM|             BELGIUM|    NYC|  2016-04-01|          Air|  NEW YORK|    2016-04-05|14.0|Pleasure|    2002.0|     F|     DL|       WT|        NEW YORK|   NY|               1.0|\n",
      "|   1466.0|2016.0|  4.0|           BELGIUM|             BELGIUM|    NYC|  2016-04-01|          Air|  NEW YORK|    2016-04-07|49.0|Pleasure|    1967.0|     F|     SN|       WT|        NEW YORK|   NY|               1.0|\n",
      "+---------+------+-----+------------------+--------------------+-------+------------+-------------+----------+--------------+----+--------+----------+------+-------+---------+----------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigrant.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Record_ID: double (nullable = true)\n",
      " |-- Year: double (nullable = true)\n",
      " |-- Month: double (nullable = true)\n",
      " |-- Country_of_Citizen: string (nullable = true)\n",
      " |-- Country_of_Residence: string (nullable = true)\n",
      " |-- Airport: string (nullable = true)\n",
      " |-- Arrival_Date: date (nullable = true)\n",
      " |-- Transfer_Mode: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Departure_Date: date (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Visa: string (nullable = true)\n",
      " |-- Birth_Year: double (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- Visa_Type: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Number_of_Arrivals: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigrant.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigrant.write.mode('overwrite').parquet(\"fact_dimension/fct_immigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "df_immigrant = spark.read.parquet(\"fact_dimension/fct_immigration.parquet\")\n",
    "df_temperature = spark.read.parquet(\"fact_dimension/dim_temperature.parquet\")\n",
    "df_airport = spark.read.parquet(\"fact_dimension/dim_airport.parquet\")\n",
    "df_demography = spark.read.parquet(\"fact_dimension/dim_demography.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed. Immigrant table contains 2154775 rows.\n",
      "Data quality check passed. Temperature table contains 248 rows.\n",
      "Data quality check passed. Airport table contains 14582 rows.\n",
      "Data quality check passed. Demography table contains 2891 rows.\n"
     ]
    }
   ],
   "source": [
    "if df_immigrant.count()>0:\n",
    "    print(\"Data quality check passed. Immigrant table contains \"+str(df_immigrant.count())+\" rows.\")\n",
    "else:\n",
    "    print(\"Data quality check failed. No data.\")\n",
    "    \n",
    "if df_temperature.count()>0:\n",
    "    print(\"Data quality check passed. Temperature table contains \"+str(df_temperature.count())+\" rows.\")\n",
    "else:\n",
    "    print(\"Data quality check failed. No data.\")\n",
    "    \n",
    "if df_airport.count()>0:\n",
    "    print(\"Data quality check passed. Airport table contains \"+str(df_airport.count())+\" rows.\")\n",
    "else:\n",
    "    print(\"Data quality check failed. No data.\")\n",
    "    \n",
    "if df_demography.count()>0:\n",
    "    print(\"Data quality check passed. Demography table contains \"+str(df_demography.count())+\" rows.\")\n",
    "else:\n",
    "    print(\"Data quality check failed. No data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "#### dim_time\n",
    " - **start_time:** date (date)\n",
    " - **hour:** integer (hour)\n",
    " - **day:** integer (day)\n",
    " - **week:** integer (week)\n",
    " - **month:** integer (month)\n",
    " - **year:** integer (year)\n",
    " - **weekday:** integer (weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_temperature\n",
    "- **City:** string (Name of city in USA)\n",
    "- **Average_Temperature:** double (Average Temperature for each city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_airport\n",
    "- **ident:** string (Unique identifier)\n",
    "- **type:** string (Type of the airport)\n",
    "- **name:** string (Airport Name)\n",
    "- **elevation_ft:** string (Altitude of the airport)\n",
    "- **continent:** string (Continent)\n",
    "- **iso_country:** string (ISO code of the country of the airport)\n",
    "- **iso_region:** string (ISO code of the country of the airport)\n",
    "- **municipality:** string (Airport City)\n",
    "- **gps_code:** string (GPS code of the airport)\n",
    "- **iata_code:** string (IATA code of the airport)\n",
    "- **local_code:** string (Local code of the airport)\n",
    "- **coordinates:** string (GPS coordinates of the airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_demography\n",
    "- **City:** string (US City)\n",
    "- **State:** string (US State)\n",
    "- **Median_Age:** string (Median Age of Population)\n",
    "- **Male_Population:** string (Number of Male Population)\n",
    "- **Total_Population:** string (Number of Total Population)\n",
    "- **Number_of_Veterans:** string (Number of Veterans)\n",
    "- **Foreign_Born:** string (Number of People who borns not in USA)\n",
    "- **Average_Household_Size:** string (Average Size of Houses)\n",
    "- **State_Code:** string (US State Code)\n",
    "- **Race:** string (Race Type)\n",
    "- **Count:** string (Number of People)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### fct_immigration\n",
    "- Record_ID: double (Unique Identifier)\n",
    "- Year: double (Year)\n",
    "- Month: double (Month)\n",
    "- Country_of_Citizen: string (Born Country)\n",
    "- Country_of_Residence: string (Residence Country)\n",
    "- Airport: string (Airport Code)\n",
    "- Arrival_Date: date (Arrival Date in USA)\n",
    "- Transfer_Mode: string (Transportation Type)\n",
    "- Address: string (Residence Address)\n",
    "- Departure_Date: date (Departure Date from USA)\n",
    "- Age: double (Age)\n",
    "- Visa: string (Visa Code)\n",
    "- Birth_Year: double (Birth Year of Immigrant)\n",
    "- Gender: string (Sex)\n",
    "- Airline: string (Airways)\n",
    "- Visa_Type: string (Visa Type of USA on Passport)\n",
    "- City: string (Airport City)\n",
    "- State: string (Airport State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Some Queries and Analysing Data\n",
    "##### Let's query on data for our questions\n",
    "First we will read our fact and dimension tables under directory which is \"fact_dimension\", in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigrant = spark.read.parquet(\"fact_dimension/fct_immigration.parquet\")\n",
    "df_temperature = spark.read.parquet(\"fact_dimension/dim_temperature.parquet\")\n",
    "df_airport = spark.read.parquet(\"fact_dimension/dim_airport.parquet\")\n",
    "df_demography = spark.read.parquet(\"fact_dimension/dim_demography.parquet\")\n",
    "\n",
    "df_immigrant.createOrReplaceTempView(\"immigration\")\n",
    "df_temperature.createOrReplaceTempView(\"temperature\")\n",
    "df_airport.createOrReplaceTempView(\"airport\")\n",
    "df_demography.createOrReplaceTempView(\"demography\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- **dim_temperature:** It consists of only U.S. cities. Here we wil use \"City\" column to join other datasets.\n",
    "- **dim_demography:** It consists of only demographic structure of U.S. cities. \"City\" and \"State\" colums will be used for joining other datasets.\n",
    "- **dim_airport:** It consists of only U.S. airports that are small, medium, large airports. Here \"municipality\" column will be joined over \"City\" columns with other datasets.\n",
    "- **dim_time:** It is time dimension tables which is created by immigrant datasets. Here we used \"arrival_date\" to create time dimension table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I will try join datasets with city or state columns. So let's see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "First let's check city temperature - number of arrivals relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I will accept 18 celcius degree as threshold. It means If city is higher than or equal 18, it is warm place else it is cold.\n",
    "So I will assign a flag for this cities in query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_test_arrival_temp_relationship = spark.sql(\"SELECT B.City, B.Average_Temperature,\\\n",
    "                    B.Number_of_Arrivals, B.Total_Arrivals, (B.Number_of_Arrivals/B.Total_Arrivals)*100 AS Ratio, \\\n",
    "                    CASE WHEN B.Average_Temperature>=18 THEN 'Warm' ELSE 'Cold' END AS Temp_Flag \\\n",
    "                    FROM ( \\\n",
    "                    SELECT A.City, A.Average_Temperature, SUM(A.Number_of_Arrivals) AS Number_of_Arrivals, SUM(A.Total_Arrivals) AS Total_Arrivals \\\n",
    "                    FROM ( \\\n",
    "                    SELECT T.City, T.Average_Temperature, SUM(F.Number_of_Arrivals) AS Number_of_Arrivals, \\\n",
    "                    (SELECT SUM(K.Number_of_Arrivals) FROM immigration K INNER JOIN temperature R ON K.City=R.City) AS Total_Arrivals \\\n",
    "                    FROM immigration F INNER JOIN temperature T ON F.City=T.City \\\n",
    "                    GROUP BY T.City, T.Average_Temperature \\\n",
    "                    ) A GROUP BY A.City, A.Average_Temperature \\\n",
    "                    ORDER BY 3 DESC \\\n",
    "                    ) B ORDER BY 3 DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------------+--------------+------------------+---------+\n",
      "|           City|Average_Temperature|Number_of_Arrivals|Total_Arrivals|             Ratio|Temp_Flag|\n",
      "+---------------+-------------------+------------------+--------------+------------------+---------+\n",
      "|       NEW YORK| 12.163888888888888|          347423.0|     1778460.0|19.535047175646344|     Cold|\n",
      "|          MIAMI|  24.44011111111111|          269994.0|     1778460.0|15.181336662055935|     Warm|\n",
      "|    LOS ANGELES| 18.120666666666665|          220832.0|     1778460.0|12.417034962833013|     Warm|\n",
      "|  SAN FRANCISCO| 16.233666666666664|          116406.0|     1778460.0|6.5453257312506326|     Cold|\n",
      "|        ORLANDO|  23.77077777777778|          115954.0|     1778460.0|  6.51991048435163|     Warm|\n",
      "|        CHICAGO|  11.58688888888889|           92069.0|     1778460.0| 5.176894616690845|     Cold|\n",
      "|        HOUSTON| 22.280222222222225|           81444.0|     1778460.0| 4.579467629297257|     Warm|\n",
      "|FORT LAUDERDALE| 24.440111111111108|           76199.0|     1778460.0| 4.284549554108611|     Warm|\n",
      "|      LAS VEGAS| 20.537999999999997|           71605.0|     1778460.0| 4.026236181865209|     Warm|\n",
      "|     WASHINGTON| 14.187666666666665|           55305.0|     1778460.0| 3.109712897675517|     Cold|\n",
      "|        ATLANTA| 16.136777777777777|           54629.0|     1778460.0| 3.071702484171699|     Cold|\n",
      "|         DALLAS|  20.45077777777778|           47012.0|     1778460.0|2.6434105911856327|     Warm|\n",
      "|         BOSTON| 10.375666666666667|           38630.0|     1778460.0|2.1721039551072274|     Cold|\n",
      "|        SEATTLE|  9.946444444444445|           32217.0|     1778460.0| 1.811511082622044|     Cold|\n",
      "|        PHOENIX| 23.564555555555554|           28660.0|     1778460.0|1.6115065843482563|     Warm|\n",
      "|        DETROIT| 10.897222222222222|           20384.0|     1778460.0|1.1461601610382015|     Cold|\n",
      "|   PHILADELPHIA| 14.013666666666667|           19402.0|     1778460.0|1.0909438502974484|     Cold|\n",
      "|          TAMPA| 23.546666666666667|           16745.0|     1778460.0|0.9415449321322942|     Warm|\n",
      "|         DENVER| 11.205777777777778|           11716.0|     1778460.0|0.6587721961697199|     Cold|\n",
      "|      CHARLOTTE| 17.705000000000002|           11534.0|     1778460.0|0.6485386233033074|     Cold|\n",
      "+---------------+-------------------+------------------+--------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_arrival_temp_relationship.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_test_arrival_temp_relationship.createOrReplaceTempView(\"arrival_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_percentage = spark.sql(\"SELECT Temp_Flag, SUM(Number_of_Arrivals), \\\n",
    "                          Total_Arrivals, \\\n",
    "                          (SUM(Number_of_Arrivals)/Total_Arrivals)*100 AS Ratio \\\n",
    "                          FROM arrival_temp \\\n",
    "                          GROUP BY Temp_Flag,Total_Arrivals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+--------------+------------------+\n",
      "|Temp_Flag|sum(Number_of_Arrivals)|Total_Arrivals|             Ratio|\n",
      "+---------+-----------------------+--------------+------------------+\n",
      "|     Cold|               838683.0|     1778460.0|47.157821935832125|\n",
      "|     Warm|               939777.0|     1778460.0|52.842178064167875|\n",
      "+---------+-----------------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_percentage.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "According to results, ratios are nearly so close. So we can't say definitely immigrants choose cities because of climate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* As you see in the project, Spark is used for extracting, transforming and load operations since it has large data processing in memory compute. Also we can use Spark in cloud environment such as AWS EMR that have distributed nodes, processing data fast.\n",
    "\n",
    "* If we had production environment, architecture would be like this below:\n",
    "\n",
    "**AWS S3 --> AWS EMR (Spark Jobs) --> Apache Airflow --> AWS Redshift**\n",
    "\n",
    "* According to data analysis of i94, monthly update is enough and we can schedule it via Apache Airflow in production environment.\n",
    "\n",
    " * **The data was increased by 100x:**\n",
    " We deploy our Spark solution to AWS EMR to compute more fast. Also cloud enviroment will scale itself if data becomes larger.\n",
    " \n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day:**\n",
    " We can use Apache Airflow to schedule and manage our data pipelines in production environment.\n",
    " \n",
    " * **The database needed to be accessed by 100+ people:**\n",
    " Our fact and dimension tables are in parquet format. So we can copy them to HDFS or AWS S3 and show them as external table by using Apache Hive. So people can connect Hive via their dashboard tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
